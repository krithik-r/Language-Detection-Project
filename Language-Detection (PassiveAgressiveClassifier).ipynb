{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415f59e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gitanjalinambiar/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/gitanjalinambiar/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/gitanjalinambiar/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/gitanjalinambiar/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/gitanjalinambiar/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674f991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gitanjalinambiar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gitanjalinambiar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002e9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee808a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id lan_code                sentence\n",
      "0   1      cmn                  我們試試看！\n",
      "1   2      cmn                 我该去睡觉了。\n",
      "2   3      cmn                 你在干什麼啊？\n",
      "3   4      cmn                  這是什麼啊？\n",
      "4   5      cmn  今天是６月１８号，也是Muiriel的生日！\n",
      "5   6      cmn           生日快乐，Muiriel！\n",
      "6   7      cmn          Muiriel现在20岁了。\n",
      "7   8      cmn           密码是\"Muiriel\"。\n",
      "8   9      cmn                我很快就會回來。\n",
      "9  10      cmn                   我不知道。\n"
     ]
    }
   ],
   "source": [
    "#load csv file\n",
    "data = pd.read_csv(\"sentences.csv\")\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b223be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON reference\n",
    "with open('lan_to_language.json') as json_file:\n",
    "    replacement_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb1b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the second column based on JSON reference\n",
    "column_to_replace = 'lan_code'  # Change this to the actual name of your second column\n",
    "for key, value in replacement_data.items():\n",
    "    data.loc[data[column_to_replace] == key, column_to_replace] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed06f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified DataFrame back to CSV\n",
    "data.to_csv('modified_sentence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502bde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cleaning\n",
    "# removing punctuations\n",
    "column_to_clean='sentence'\n",
    "data[column_to_clean] = data[column_to_clean].str.translate(str.maketrans(\"\", \"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be300b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing numbers\n",
    "data[column_to_clean] = data[column_to_clean].astype(str).str.replace('\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a2490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special characters\n",
    "data[column_to_clean] = data[column_to_clean].str.replace('[^a-zA-Z\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data[column_to_clean] = data[column_to_clean].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97727e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing null values\n",
    "data = data.dropna(subset=[column_to_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c068e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"modified_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5209cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id          lan_code                sentence\n",
      "0   1  Mandarin Chinese                  我們試試看！\n",
      "1   2  Mandarin Chinese                 我该去睡觉了。\n",
      "2   3  Mandarin Chinese                 你在干什麼啊？\n",
      "3   4  Mandarin Chinese                  這是什麼啊？\n",
      "4   5  Mandarin Chinese  今天是６月１８号，也是Muiriel的生日！\n",
      "5   6  Mandarin Chinese           生日快乐，Muiriel！\n",
      "6   7  Mandarin Chinese          Muiriel现在20岁了。\n",
      "7   8  Mandarin Chinese           密码是\"Muiriel\"。\n",
      "8   9  Mandarin Chinese                我很快就會回來。\n",
      "9  10  Mandarin Chinese                   我不知道。\n"
     ]
    }
   ],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67769529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "lan_code    0\n",
       "sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum() gives the count of all the null values\n",
    "#count() gives the count non-null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beccebf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>我們試試看！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>我该去睡觉了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>你在干什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>這是什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>今天是６月１８号，也是Muiriel的生日！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341807</th>\n",
       "      <td>10794524</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Quiero este libro por favor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341808</th>\n",
       "      <td>10794525</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Los han hecho huir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341809</th>\n",
       "      <td>10794526</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Los botaron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341810</th>\n",
       "      <td>10794527</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Los hicieron correr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341811</th>\n",
       "      <td>10794528</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Los corrieron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10330761 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id          lan_code                      sentence\n",
       "0                1  Mandarin Chinese                        我們試試看！\n",
       "1                2  Mandarin Chinese                       我该去睡觉了。\n",
       "2                3  Mandarin Chinese                       你在干什麼啊？\n",
       "3                4  Mandarin Chinese                        這是什麼啊？\n",
       "4                5  Mandarin Chinese        今天是６月１８号，也是Muiriel的生日！\n",
       "...            ...               ...                           ...\n",
       "10341807  10794524           Spanish  Quiero este libro por favor.\n",
       "10341808  10794525           Spanish           Los han hecho huir.\n",
       "10341809  10794526           Spanish                  Los botaron.\n",
       "10341810  10794527           Spanish          Los hicieron correr.\n",
       "10341811  10794528           Spanish                Los corrieron.\n",
       "\n",
       "[10330761 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f8015ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lan_code\n",
       "English             1586621\n",
       "Russian              909951\n",
       "Italian              805104\n",
       "Turkish              717897\n",
       "Esperanto            685643\n",
       "                     ...   \n",
       "Southern Haida            1\n",
       "Rendille                  1\n",
       "Louisiana Creole          1\n",
       "Nyunga                    1\n",
       "Cuyonon                   1\n",
       "Name: count, Length: 404, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"lan_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff8931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(data[\"sentence\"])\n",
    "y=np.array(data[\"lan_code\"])\n",
    "\n",
    "cv=CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33,random_state=42) \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce23f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf63da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PassiveAggressiveClassifier(max_iter=1000, random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "predictions=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9679a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9641505298584915"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6888626",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b0855e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3495.5179409980774\n"
     ]
    }
   ],
   "source": [
    "total_time=end_time-start_time\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eb0e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Text: 안녕하세요. 잘 지내죠\n",
      "['Korean']\n"
     ]
    }
   ],
   "source": [
    "user = input(\"Enter a Text: \")\n",
    "data = cv.transform([user]).toarray()\n",
    "output = model.predict(data)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
